{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LocalRecall\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "from PIL import ImageGrab\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I know its local, so we dont need to worry about the security of the key, but i will use the .env file to store the key anyway\n",
    "def getConections():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"postgres\",\n",
    "        user= os.getenv('user'),\n",
    "        password= os.getenv('password')\n",
    "    )\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "    register_vector(conn)\n",
    "    return conn, cur\n",
    "conn, cur = getConections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR COMPARISION\n",
    "def getIndex():\n",
    "    from pinecone import Pinecone\n",
    "    pc = Pinecone(api_key=os.getenv('pineKey'))\n",
    "    index = pc.Index(\"vectors\")\n",
    "    return index\n",
    "index = getIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProcessor():\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    return processor\n",
    "\n",
    "def getProcImage(imgs: np.ndarray):\n",
    "    processor = getProcessor()\n",
    "    size = {'height': 224, 'width': 224}\n",
    "    proc_img = processor(images=imgs, size = size , return_tensors=\"pt\")\n",
    "    return proc_img\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     s1 = ImageGrab.grab(); s1 = np.asarray(s1)\n",
    "#     s2 = ImageGrab.grab(); s2 = np.asarray(s2)\n",
    "\n",
    "#     imgs = np.array([s1, s2])\n",
    "#     procImages = getProcImage(imgs)\n",
    "#     s1p = procImages.get('pixel_values')[0]\n",
    "#     s2p = procImages.get('pixel_values')[1]\n",
    "\n",
    "#     #Rudamentary difference check\n",
    "#     # May need to implement a more robust method like PySceneDetect\n",
    "#     diff = torch.abs(s1p - s2p).sum()\n",
    "    \n",
    "#     if int(diff) > 2000:\n",
    "#         s2 = Image.fromarray(s2)\n",
    "#         s2.save(f'snaps/{time.time()}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8998773097991943\n"
     ]
    }
   ],
   "source": [
    "def prepMeta(dir : str = 'snaps'):\n",
    "    files = os.listdir(dir)\n",
    "    meta = []\n",
    "    for file in files:\n",
    "        meta.append(file.split('.png')[0].strip())\n",
    "    return meta\n",
    "\n",
    "def getClipModel():\n",
    "    return CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "s = time.time() \n",
    "model = getClipModel()\n",
    "print(time.time() - s)\n",
    "\n",
    "\n",
    "def getEmbeddings(dirMode: bool = False, imageTensors: np.ndarray = None, model = None):\n",
    "    model = getClipModel() if model is None else model\n",
    "    if dirMode:\n",
    "        meta = prepMeta(dir = 'snaps')\n",
    "        imageNdarray = []\n",
    "        imageNdarray.extend([Image.open(f'snaps/{m}.png') for m in meta])\n",
    "        batch_tensor = getProcImage(imageNdarray)\n",
    "        batch_tensor = batch_tensor.get('pixel_values')\n",
    "          \n",
    "    else:\n",
    "        if imageTensors is None:\n",
    "            raise Exception('No images found in array')\n",
    "        batch_tensor = torch.stack(imageTensors)\n",
    "\n",
    "    inputs = {'pixel_values': batch_tensor}\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model.get_image_features(**inputs)\n",
    "\n",
    "    documents = []\n",
    "    for i, m in enumerate(meta):\n",
    "        documents.append({'id': m, 'vector': output[i].tolist()})\n",
    "    return documents\n",
    "\n",
    "documents = getEmbeddings(dirMode = True, model = model)\n",
    "df = pd.DataFrame(documents)\n",
    "df.to_csv('embeddings.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTable(name = 'vectors'):\n",
    "    cur.execute(f'''CREATE TABLE IF NOT EXISTS {name}(\n",
    "        id VARCHAR PRIMARY KEY,\n",
    "        ts float8,\n",
    "        vector vector(512)\n",
    "    )''')\n",
    "    conn.commit()\n",
    "createTable(\"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertVectors(documents, insertOne = False):\n",
    "    if insertOne:\n",
    "        cur.execute(\"INSERT INTO vectors (id, vector, ts) VALUES (%s, %s, %s)\", (documents.get('id'), documents.get('vector'), documents.get('id')))\n",
    "        conn.commit()\n",
    "\n",
    "    data_list = [(doc.get('id'), doc.get('vector'), doc.get('id')) for doc in documents]\n",
    "    execute_values(cur, \"INSERT INTO vectors (id, vector, ts) VALUES %s\", data_list)\n",
    "    conn.commit()\n",
    "\n",
    "# conn, cur = getConections()\n",
    "insertVectors(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 512,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10}},\n",
       " 'total_vector_count': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pineConeDocs = [{'id': doc.get('id'), 'values': doc.get('vector')} for doc in documents]\n",
    "index.upsert(pineConeDocs)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllVectors(ids = None):\n",
    "    if ids is None:\n",
    "        cur.execute(\"SELECT * FROM vectors\")\n",
    "        \n",
    "    elif isinstance(id, list):\n",
    "        cur.execute(f\"SELECT * FROM vectors WHERE id IN {tuple(id)}\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    return rows\n",
    "conn, cur = getConections()\n",
    "res = getAllVectors()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative text found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pgVector': [('1716656818.5502412', 0.06367490742784743),\n",
       "  ('1716655845.6821773', 0.05001505844142251)],\n",
       " 'pineCone': {'matches': [{'id': '1716656818.5502412', 'score': 0.0625313669, 'values': []},\n",
       "              {'id': '1716655845.6821773', 'score': 0.050085403, 'values': []}],\n",
       "  'namespace': '',\n",
       "  'usage': {'read_units': 5}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dotQuery(inputText:str = None, ids:list = None, topk = 1, includeDistance = False, \n",
    "             includeVector = False, metadataFilter:list = None, inId:list = None, pineCone = False,\n",
    "             negative_text = None):\n",
    "    \n",
    "    if inputText is None:\n",
    "        raise Exception('No input text found')\n",
    "    \n",
    "    processor = getProcessor()\n",
    "    inputs = processor(text = inputText, return_tensors=\"pt\", padding=True)\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    if negative_text:\n",
    "        print('Negative text found')\n",
    "        inputs = processor(text = negative_text, return_tensors=\"pt\", padding=True)\n",
    "        neg_features = model.get_text_features(**inputs)\n",
    "        neg_features = neg_features / neg_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features - neg_features\n",
    "\n",
    "        \n",
    "    text_features = text_features.tolist()[0]\n",
    "    \n",
    "    if pineCone:\n",
    "        pineres = index.query(vector = text_features, top_k = topk, \n",
    "                          include_values = includeVector, \n",
    "                          include_metadata= True if metadataFilter else False)\n",
    "\n",
    "    base_query = \"SELECT \"\n",
    "    \n",
    "    # Select fields based on includeDistance and includeVector\n",
    "    fields = []\n",
    "    if includeVector:\n",
    "        fields.append(\"id, vector\")\n",
    "    else:\n",
    "        fields.append(\"id\")\n",
    "    \n",
    "    if includeDistance:\n",
    "        # Cosine similarity, higher is closer\n",
    "        fields.append(f\" 1 - (vector <=> '{text_features}'::vector) AS cosine_similarity\")\n",
    "    \n",
    "    if metadataFilter:\n",
    "        fields.extend(metadataFilter)\n",
    "    \n",
    "    base_query += \", \".join(fields) + \" FROM vectors\"\n",
    "\n",
    "    if inId is not None:\n",
    "        if len(inId) == 1:\n",
    "            in_clause = f\"WHERE id = '{inId[0]}'\"\n",
    "        else:\n",
    "            in_clause = f\"WHERE id IN {tuple(inId)}\"\n",
    "        base_query += \" \" + in_clause\n",
    "    \n",
    "    # Order by distance\n",
    "    base_query += f\" ORDER BY vector <=> '{text_features}'::vector LIMIT {topk}\"\n",
    "\n",
    "    query = base_query\n",
    "    \n",
    "    cur.execute(query)\n",
    "    x = cur.fetchall()\n",
    "\n",
    "    if includeDistance:\n",
    "        x = sorted(x, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if pineCone:\n",
    "        res = {\n",
    "            'pgVector': x,\n",
    "            'pineCone': pineres\n",
    "        }\n",
    "        return res\n",
    "    \n",
    "    return {\n",
    "        'pgVector': x\n",
    "    }\n",
    "\n",
    "conn, cur = getConections()\n",
    "dotQuery(inputText = \"snapshot of a white screen with blue and black text \", negative_text= \"has blue background\" ,topk = 2, includeDistance = True, includeVector = False, pineCone = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(\"DELETE FROM vectors\")\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
